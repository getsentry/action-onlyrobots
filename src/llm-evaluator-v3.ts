import OpenAI from 'openai';

export interface LLMEvaluationResult {
  isHumanLike: boolean;
  confidence: number;
  reasoning: string;
  indicators: string[];
}

export interface FileToEvaluate {
  filename: string;
  patch: string;
}

export interface PRContext {
  title?: string;
  description?: string;
  commitMessages?: string[];
  author?: string;
}

export class LLMEvaluator {
  private openai: OpenAI;

  constructor(config: { OPENAI_API_KEY: string }) {
    if (!config.OPENAI_API_KEY) {
      throw new Error('OPENAI_API_KEY is required');
    }

    this.openai = new OpenAI({
      apiKey: config.OPENAI_API_KEY,
    });
  }

  async evaluatePullRequest(
    files: FileToEvaluate[],
    prContext?: PRContext
  ): Promise<LLMEvaluationResult> {
    const prompt = this.buildPrompt(files, prContext);

    try {
      const response = await this.openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: AI_DETECTION_PROMPT_V3,
          },
          {
            role: 'user',
            content: prompt,
          },
        ],
        temperature: 0.1,
        max_tokens: 1000,
      });

      const content = response.choices[0]?.message?.content;
      if (!content) {
        throw new Error('No response from LLM');
      }

      return this.parseResponse(content);
    } catch (error) {
      console.error('LLM evaluation error:', error);
      return {
        isHumanLike: true,
        confidence: 50,
        reasoning: `Evaluation error: ${error}. Defaulting to human authorship.`,
        indicators: ['evaluation-error'],
      };
    }
  }

  private buildPrompt(files: FileToEvaluate[], prContext?: PRContext): string {
    const parts: string[] = [];

    parts.push('**Pull Request Details:**');
    if (prContext?.title) parts.push(`Title: ${prContext.title}`);
    if (prContext?.author) parts.push(`Author: ${prContext.author}`);
    if (prContext?.description) {
      parts.push(`\nDescription:\n${prContext.description}`);
    }
    if (prContext?.commitMessages?.length) {
      parts.push(`\nCommit Messages:`);
      prContext.commitMessages.forEach((msg, i) => {
        parts.push(`${i + 1}. ${msg}`);
      });
    }

    parts.push(`\n**Code Changes (${files.length} file${files.length !== 1 ? 's' : ''}):**\n`);
    files.forEach((file, index) => {
      parts.push(`File ${index + 1}: ${file.filename}`);
      parts.push('```diff');
      parts.push(file.patch);
      parts.push('```\n');
    });

    return parts.join('\n');
  }

  private parseResponse(content: string): LLMEvaluationResult {
    try {
      const parsed = JSON.parse(content);
      return {
        isHumanLike: parsed.isHumanLike ?? true,
        confidence: parsed.confidence ?? 50,
        reasoning: parsed.reasoning ?? 'Unable to parse reasoning',
        indicators: parsed.indicators ?? [],
      };
    } catch {
      const isHumanLike = !content.toLowerCase().includes('ai-generated');
      const confidenceMatch = content.match(/confidence[:\s]*(\d+)/i);
      const confidence = confidenceMatch ? parseInt(confidenceMatch[1]) : 50;

      return {
        isHumanLike,
        confidence,
        reasoning: content,
        indicators: [],
      };
    }
  }
}

const AI_DETECTION_PROMPT_V3 = `You are an expert at detecting AI-generated code pull requests. You must analyze the ENTIRE pull request holistically - description, commits, and code changes together.

# Critical Detection Patterns

## 1. DEFINITIVE AI SIGNALS (95%+ confidence if found)
- **Bot authors**: Username ends with [bot] (e.g., "dependabot[bot]", "seer-by-sentry[bot]")
- **AI signatures**: "ü§ñ Generated with [Claude Code]", "Co-Authored-By: Claude <noreply@anthropic.com>"
- **Explicit attribution**: "generated by Seer", "Created with Cursor", "AI-assisted"
- **AI emojis**: ü§ñ, üëÅÔ∏è (Seer), ü§ñ in commits

## 2. STRUCTURED PR DESCRIPTIONS (90% confidence when combined)
AI-generated PRs have VERY specific description patterns:

### Example AI Pattern:
\`\`\`
## Summary
- Clear bullet point
- Another clear point
- Third comprehensive point

## Changes
### Features
- Feature description
### Bug Fixes  
- Fix description
### Test Updates
- Test description

## Test plan
- [x] First test item
- [x] Second test item
- [x] All checks pass
\`\`\`

**Key indicators:**
- Multiple markdown sections (##) with standard names: Summary, Changes, Test plan, Impact
- Checkbox lists with [x] 
- Sub-sections with ### 
- Perfectly formatted bullet points
- Comprehensive coverage of all aspects

## 3. COMPREHENSIVE CODE CHANGES (85% confidence)
AI makes systematic, complete changes:
- **All related files updated**: If changing a function, AI updates the function, tests, types, and docs
- **Perfect test coverage**: AI adds comprehensive tests covering edge cases
- **Consistent patterns**: Same refactoring applied uniformly across all files
- **No iterative development**: Everything works perfectly on first try

## 4. COMMIT MESSAGE PATTERNS 
**AI Pattern (80% confidence):**
- **Every commit** follows conventional format: "feat:", "fix:", "chore:", "docs:"
- Multi-line commits with perfect formatting
- Detailed explanations in commit body
- No typos, no casual messages like "wip" or "updates"

**IMPORTANT: Iterative commits suggest HUMAN authorship:**
- Multiple "fix:" commits trying to get something working
- "fix: X", then "fix: actually fix X", then "fix: restore X"
- This shows trial-and-error, which AI doesn't do

## 5. CODE CHARACTERISTICS (75% confidence when multiple present)
- **Over-engineering**: Comprehensive error handling for simple functions
- **Perfect naming**: userAccountInformation, formatUserDisplayNameWithEmailAddress
- **Excessive comments**: Every function has JSDoc, obvious code is explained
- **No debugging artifacts**: No console.log, no commented code, no TODOs
- **Uniform style**: Exact same patterns throughout all files

# Human Indicators (increase confidence in human authorship)
- **Minimal descriptions**: Empty or single line
- **Targeted fixes**: Only changes what's broken, nothing more
- **Inconsistent style**: Mixed formatting, naming conventions
- **Iterative commits**: "fix", "actually fix", "try again"
- **Debugging remnants**: console.log, commented attempts, TODOs
- **Pragmatic solutions**: Quick fixes, hardcoded values when appropriate

# CRITICAL EVALUATION RULES

1. **Bot authors are ALWAYS AI** - No exceptions (e.g., dependabot[bot], seer-by-sentry[bot])
2. **Explicit AI signatures override everything** - "ü§ñ Generated with", "Co-Authored-By: Claude"
3. **Balance ALL signals together**:
   - Structured description + perfect commits + comprehensive changes = AI
   - Structured description + iterative commits + small change = Could be human using AI for description
   - No description + targeted fix + inconsistent commits = Human
4. **Iterative development is HUMAN** - Multiple attempts, fixes to fixes, trial and error
5. **When uncertain, default to HUMAN** - Need strong evidence for AI classification

# Examples to recognize:

**AI-Generated PR Description:**
- Has ## Summary, ## Changes, ## Test plan sections
- Uses - [x] checkbox syntax
- Perfectly formatted markdown
- Covers every aspect comprehensively

**Human PR Description:**
- "Fix bug in login"
- Empty description
- Single paragraph without structure
- Focuses only on the main change

**Response Format (JSON):**
{
  "isHumanLike": boolean,
  "confidence": number (0-100),
  "reasoning": "Explain the most important patterns you found",
  "indicators": ["bot-author", "structured-description", "perfect-commits", etc.]
}

Remember: Many AI PRs lack explicit signatures. Focus on the PATTERNS, not just attribution text.`;
